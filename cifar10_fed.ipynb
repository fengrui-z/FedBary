{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:54:03.631834Z",
     "start_time": "2024-03-24T10:54:00.382031Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/xxf737494996yh16lftvfp2c0000gn/T/ipykernel_20362/4210827980.py:11: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/Users/muz1lee/PycharmProjects/misclaim/otdp/otdd/pytorch/utils.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ot\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from otdd.pytorch.moments import *\n",
    "from otdd.pytorch.utils import *\n",
    "\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def euclidean_dist_torch(x1, x2):\n",
    "    x1p = x1.pow(2).sum(1).unsqueeze(1)\n",
    "    x2p = x2.pow(2).sum(1).unsqueeze(1)\n",
    "    prod_x1x2 = torch.mm(x1, x2.t())\n",
    "    distance = x1p.expand_as(prod_x1x2) + \\\n",
    "        x2p.t().expand_as(prod_x1x2) - 2*prod_x1x2\n",
    "    return torch.sqrt(distance)  # /x1.size(0)/x2.size(0)\n",
    "\n",
    "\n",
    "def get_interp_measure(xs,xt,G0,t,thresh=1e-5):\n",
    "    \"\"\" Get an exact interpolating measure between xs, xt \n",
    "    given the transport plan G0 and $t$.\n",
    "\n",
    "    Args:\n",
    "        xs (array): _description_\n",
    "        xt (array): _description_\n",
    "        G0 (_type_): _description_\n",
    "        t (scalar real): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    n_s, dim = xs.shape\n",
    "    n_t = xt.shape[0]\n",
    "    xsp = np.zeros((n_s+n_t+1, dim))\n",
    "    xtp = np.zeros((n_s+n_t+1, dim))\n",
    "    weights = np.zeros((n_s+n_t+1,))\n",
    "    k = 0\n",
    "    for i in range(xs.shape[0]):\n",
    "        ind = np.where(G0[i, :]>thresh)[0]\n",
    "        for j in range(len(ind)):\n",
    "            xsp[k,:] = xs[i, :]\n",
    "            xtp[k,:] = xt[ind[j], :]\n",
    "            weights[k] = G0[i,ind[j]]\n",
    "            k += 1\n",
    "    # if k > n_s:\n",
    "    #     print(k, n_s)\n",
    "    #     pass\n",
    "    xsp = xsp[:k, :]\n",
    "    xtp = xtp[:k, :]\n",
    "    xz = (1-t)*xsp + t*xtp\n",
    "    weights = weights[:k]/np.sum(weights[:k])\n",
    "    #print(xz.shape, weights.shape)\n",
    "    \n",
    "    return xz, weights\n",
    "\n",
    "def interp_meas(X,Y,t_val=None,metric='sqeuclidean',approx_interp=True,\n",
    "                a = None,b = None):\n",
    "    \"\"\"\n",
    "    compute an the OT plan, cost and an interpolating measure\n",
    "    works for squared euclidean distance\n",
    "    everything is done on numpy\n",
    "    \n",
    "    return \n",
    "        * the interpolating measure\n",
    "        * the OT cost between X and Y\n",
    "        * the transport plan\n",
    "    \"\"\"\n",
    "    nx, ny  = X.shape[0], Y.shape[0]\n",
    "    p = 2 if metric=='sqeuclidean' else 1  \n",
    "    if a is None:  \n",
    "        a = np.ones((nx,),dtype=np.float64) / nx\n",
    "    if b is None:\n",
    "        b = np.ones((ny,),dtype=np.float64) / ny  \n",
    "    # loss matrix\n",
    "    M = ot.dist(X,Y,metric=metric) # squared euclidean distance 'default'\n",
    "    # compute EMD\n",
    "    norm = np.max(M) if np.max(M)>1 else 1\n",
    "    G0 = ot.emd(a, b, M/norm)\n",
    "    \n",
    "    \n",
    "    t = np.random.rand(1) if t_val==None else t_val\n",
    "    #print('t',t)\n",
    "    if approx_interp:\n",
    "        Z = (1-t)*X + t*(G0*nx)@Y\n",
    "        weights =  np.ones((nx,),dtype=np.float64) / nx\n",
    "    else:\n",
    "        Z, weights = get_interp_measure(X,Y,G0,t)\n",
    "    cost = np.sum(G0*M)**(1/p)\n",
    "    return Z, weights, cost, G0\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def learn_interp_meas_support(xs,xt,n_supp=100,n_epoch=100,\n",
    "                                t_val = None, lr= 0.01,p=2,\n",
    "                                z_init=None, verbose=False,\n",
    "                                a = None, b = None):\n",
    "    \"\"\"\n",
    "    xs and xt are supposed to be numpy arrays\n",
    "    \n",
    "    p = 2 squared euclidean distance\n",
    "    p = 1 euclidean distance\n",
    "    \n",
    "    output are numpy arrays \n",
    "    \"\"\"\n",
    "    if t_val is None:\n",
    "        t_val = np.random.rand(1)[0] if t_val==None else t_val\n",
    "    # TODO: add numpy transformation of xs and xt\n",
    "    \n",
    "    dim = xs.shape[1]\n",
    "    c = np.ones(n_supp)/n_supp\n",
    "    z = nn.Embedding(n_supp, dim)\n",
    "    if z_init is not None:\n",
    "        z.weight.data = torch.from_numpy(z_init)\n",
    "    else:\n",
    "        z.weight.data = torch.ones(n_supp, dim)\n",
    "    z_init = z.weight.detach().clone()\n",
    "    ns = xs.shape[0]\n",
    "    nt = xt.shape[0]\n",
    "    if a is None:  \n",
    "        a = np.ones((ns,),dtype=np.float64) / ns\n",
    "    if b is None:\n",
    "        b = np.ones((nt,),dtype=np.float64) / nt \n",
    "    optimizer = optim.Adam(z.parameters(), lr=lr)\n",
    "    s_list = []\n",
    "    #print('learn',t_val)\n",
    "    for i in range(n_epoch):\n",
    "        # computing distance matrices \n",
    "        # between samples and interpolating measure\n",
    "\n",
    "        Ms = euclidean_dist_torch(torch.from_numpy(xs).double(), z.weight.double()).pow(p)\n",
    "        Mt = euclidean_dist_torch( z.weight.double(), torch.from_numpy(xt).double()).pow(p)\n",
    "        with torch.no_grad():\n",
    "            Ms_aux =  Ms.detach().data.numpy()\n",
    "            Mt_aux =  Mt.detach().data.numpy()\n",
    "            normMs = np.max(Ms_aux) if np.max(Ms_aux)>1 else 1\n",
    "            normMt = np.max(Mt_aux) if np.max(Mt_aux)>1 else 1\n",
    "\n",
    "            gamma_s = ot.emd(a, c, Ms_aux/normMs)\n",
    "            gamma_s = torch.from_numpy(gamma_s)\n",
    "            gamma_t = ot.emd(c,b, Mt_aux/normMt)\n",
    "            gamma_t = torch.from_numpy(gamma_t)\n",
    "        S = (1-t_val)*(torch.sum(Ms*gamma_s)).pow(1/p) + t_val*(torch.sum(Mt*gamma_t)).pow(1/p)\n",
    "        z.zero_grad()\n",
    "        S.backward()\n",
    "        s_list.append(S.item())\n",
    "        optimizer.step()\n",
    "    cost = (torch.sum(Ms*gamma_s)).pow(1/p) + (torch.sum(Mt*gamma_t)).pow(1/p)\n",
    "    z = z.weight.detach().numpy()\n",
    "    # TODO: change plan to the full plan from X to Y\n",
    "    return z, cost.detach().item(), [gamma_s,gamma_t], s_list\n",
    "\n",
    "\n",
    "\n",
    "class InterpMeas:\n",
    "    def __init__(self,metric='sqeuclidean',t_val=None,approx_interp=True,\n",
    "                 learn_support=False):\n",
    "        self.metric = metric\n",
    "        self.t_val = t_val\n",
    "        self.n_supp = 100\n",
    "        self.approx_interp = approx_interp\n",
    "\n",
    "        #-- useful for learning support\n",
    "        self.lr = 0.01\n",
    "        self.n_epoch = 100\n",
    "        self.int_init = None\n",
    "        self.learn_support = learn_support\n",
    "    def fit(self,X,Y, a=None, b=None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            X (np_array): size nx x dim\n",
    "            Y (np_array): _description_\n",
    "            a (np_array, optional): _weights of the empirical distribution X . Defaults to None with equal weights.\n",
    "            b (np_array, optional): _weights of the empirical distribution X . Defaults to None with equal weights.\n",
    "            \n",
    "        Returns:\n",
    "            An InterpMeas object with the following attributes:\n",
    "            int_m (np_array): size n_supp x dim\n",
    "            weights (np_array): size n_supp x 1\n",
    "            plan (np_array): size nx x nt\n",
    "            loss_learn (list): list of the loss function during the learning of the support\n",
    "            cost (float): cost of the optimal transport plan\n",
    "        \"\"\"\n",
    "        t = np.random.rand(1)[0] if self.t_val==None else self.t_val\n",
    "        if not self.learn_support:\n",
    "            Z, weights, cost, G0 = interp_meas(X,Y,t_val=t,metric=self.metric,\n",
    "                                               a=a,b=b,approx_interp=self.approx_interp)\n",
    "            self.t = t\n",
    "            self.int_m = Z \n",
    "            self.cost = cost\n",
    "            self.plan = G0\n",
    "            self.weights = weights\n",
    "        elif self.learn_support:\n",
    "            t = np.random.rand(1)[0] if self.t_val==None else self.t_val\n",
    "            p = 2 if self.metric=='sqeuclidean' else 1    \n",
    "            Z, cost, gamma, s_list = learn_interp_meas_support(X,Y,n_supp=self.n_supp,n_epoch=self.n_epoch,\n",
    "                                t_val = t, lr= self.lr, p=p,\n",
    "                                z_init= self.int_init,\n",
    "                                a=a, b = b)\n",
    "            self.int_m = Z\n",
    "            self.weights = np.ones((Z.shape[0],),dtype=np.float64) / Z.shape[0]\n",
    "            self.cost =  cost\n",
    "            # TODO: change plan to the full plan from X to Y\n",
    "            self.plan = gamma\n",
    "            self.loss_learn = s_list\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FedOT:\n",
    "    def __init__(self, n_supp,n_epoch, t_val=None,verbose=False,\n",
    "                 get_int_list=False,\n",
    "                 metric = 'sqeuclidean'):\n",
    "        self.n_supp = n_supp  # n_supp of the interpolating measure\n",
    "        self.n_epoch = n_epoch\n",
    "        self.t_val = t_val\n",
    "        self.verbose = verbose\n",
    "        self.get_int_list = get_int_list\n",
    "        self.metric = metric\n",
    "        self.random_val_init = 1\n",
    "        if self.metric == 'sqeuclidean':\n",
    "            self.p=2\n",
    "        elif self.metric == 'euclidean':\n",
    "            self.p=1 \n",
    "\n",
    "    def fit(self,xs, xt, ws = None, wt = None,approx_interp=True,\n",
    "            learn_support=False):   \n",
    "        self.approx_interp = approx_interp\n",
    "        self.learn_support = learn_support\n",
    "        dim = xs.shape[1]\n",
    "        cost_diff = 0\n",
    "        istensor = False\n",
    "\n",
    "        # xs A ,  xt B , server C\n",
    "        if type(xs) == torch.Tensor:\n",
    "            xs_ = torch.clone(xs) \n",
    "            xs= xs.detach().numpy() # (100,2000)\n",
    "            xt_ = torch.clone(xt) \n",
    "            xt= xt.detach().numpy() #(300,2000)\n",
    "            istensor = True\n",
    "            if ws is not None:\n",
    "                ws = ws.numpy().astype(np.float64)\n",
    "            else :\n",
    "                ws = np.ones((xs.shape[0],),dtype=np.float64) / xs.shape[0]\n",
    "                \n",
    "            if wt is not None:\n",
    "                wt = wt.numpy().astype(np.float64)\n",
    "            else :\n",
    "                wt = np.ones((xt.shape[0],),dtype=np.float64) / xt.shape[0]\n",
    "        # creating object for interpolation\n",
    "        interp_G = InterpMeas(metric=self.metric,t_val=self.t_val,approx_interp=approx_interp,\n",
    "                              learn_support=self.learn_support)\n",
    "        interp_H = InterpMeas(metric=self.metric,t_val=self.t_val,approx_interp=approx_interp,\n",
    "                              learn_support=self.learn_support)\n",
    "        interp_m = InterpMeas(metric=self.metric,t_val=self.t_val,approx_interp=approx_interp,\n",
    "                              learn_support=self.learn_support)\n",
    "\n",
    "        int_m = np.random.randn(self.n_supp,dim)*self.random_val_init  # (50,2000)\n",
    "        weight_int_m = np.ones(self.n_supp)/self.n_supp\n",
    "\n",
    "        list_cost = []\n",
    "        list_int_m = []\n",
    "        list_int_G = []\n",
    "        list_int_H = []\n",
    "        \n",
    "        for i in range(self.n_epoch):\n",
    "            if self.verbose:\n",
    "                print(i)\n",
    "            if self.get_int_list:\n",
    "                list_int_m.append(int_m)\n",
    "\n",
    "            # xs-- G.int_m --int_m  -- H.int_m -- xt\n",
    "\n",
    "            # on client S\n",
    "            interp_G.fit(int_m,xs,a=weight_int_m, b=ws)\n",
    "            G, weight_G, cost_g= interp_G.int_m, interp_G.weights, interp_G.cost\n",
    "            interp_G.int_init = G\n",
    "            # on client T\n",
    "            interp_H.fit(int_m,xt, a=weight_int_m, b=wt)\n",
    "            H, weight_H, cost_h = interp_H.int_m, interp_H.weights, interp_H.cost\n",
    "            interp_H.int_init = H\n",
    "            # send costs, G and H to the server\n",
    "            # on server\n",
    "            list_cost.append( cost_g+ cost_h)\n",
    "            interp_m = interp_m.fit(H, G,a=weight_H,b=weight_G)\n",
    "            int_m, weight_int_m = interp_m.int_m, interp_m.weights\n",
    "            interp_m.int_init = int_m.copy()\n",
    "            if self.get_int_list:\n",
    "                list_int_G.append(G)\n",
    "                list_int_H.append(H)\n",
    "        # preparing output for differentiable cost\n",
    "        if istensor:\n",
    "            eps = 1e-6\n",
    "\n",
    "            Ms = euclidean_dist_torch(xs_.double(), torch.from_numpy(int_m).double()).pow(self.p)\n",
    "            Mt = euclidean_dist_torch(torch.from_numpy(int_m).double(), xt_.double()).pow(self.p)\n",
    "         \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                ns, nt = xs_.shape[0], xt_.shape[0]\n",
    "                nm = int_m.shape[0]\n",
    "                c = weight_int_m\n",
    "                Ms_aux =  Ms.detach().data.numpy()\n",
    "                Mt_aux =  Mt.detach().data.numpy()\n",
    "                normMs = np.max(Ms_aux) if np.max(Ms_aux)>1 else 1\n",
    "                normMt = np.max(Mt_aux) if np.max(Mt_aux)>1 else 1\n",
    "                #print(np.sum(a),np.sum(b),np.sum(c),)\n",
    "                gamma_s = ot.emd(ws, c, Ms_aux/normMs)\n",
    "                planS = torch.from_numpy(gamma_s)\n",
    "                gamma_t = ot.emd(c,wt, Mt_aux/normMt)\n",
    "                planT = torch.from_numpy(gamma_t)\n",
    "            cost = (torch.sum(Ms*planS)+ eps)**(1/self.p) + \\\n",
    "                         (torch.sum(Mt*planT)+ eps)**(1/self.p) \n",
    "        else:\n",
    "            nt = xt.shape[0]\n",
    "            interp_G.fit(xs,int_m)\n",
    "            G, weight_G, cost_g, planS = interp_G.int_m, interp_G.weights, interp_G.cost, interp_G.plan\n",
    "            interp_H.fit(int_m,xt)\n",
    "            H, weight_G, cost_h, planT = interp_H.int_m, interp_H.weights, interp_H.cost, interp_H.plan\n",
    "            cost = cost_g + cost_h\n",
    "\n",
    "        \n",
    "        self.int_meas = int_m\n",
    "        self.weights = weight_int_m\n",
    "        self.list_cost = list_cost\n",
    "        self.cost = cost \n",
    "        self.cost_g = cost_g\n",
    "        self.cost_h = cost_h\n",
    "        self.planS, self.planT = planS, planT\n",
    "        self.plan = planS@planT*nt\n",
    "        self.list_int_meas = list_int_m\n",
    "        self.list_int_G = list_int_G\n",
    "        self.list_int_H = list_int_H\n",
    "\n",
    "        return self\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:54:06.571140Z",
     "start_time": "2024-03-24T10:54:06.500690Z"
    }
   },
   "id": "b721639cad4372ef"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class direct_learn:\n",
    "    def __init__(self,metric='sqeuclidean',t_val=None,approx_interp=True,\n",
    "                 learn_support=False):\n",
    "        self.metric = metric\n",
    "        self.t_val = t_val\n",
    "        self.n_supp = 100\n",
    "        self.approx_interp = approx_interp\n",
    "\n",
    "        #-- useful for learning support\n",
    "        self.lr = 0.01\n",
    "        self.n_epoch = 100\n",
    "        self.int_init = None\n",
    "        self.learn_support = learn_support\n",
    "        if self.metric == 'sqeuclidean':\n",
    "            self.p=2\n",
    "        elif self.metric == 'euclidean':\n",
    "            self.p=1 \n",
    "\n",
    "    def fit_direct(self,xs, xt, ws = None,wt = None,approx_interp=True,\n",
    "                learn_support=False):   \n",
    "            self.approx_interp = approx_interp\n",
    "            self.learn_support = learn_support\n",
    "\n",
    "\n",
    "            istensor = False\n",
    "    \n",
    "\n",
    "            if type(xs) == torch.Tensor:\n",
    "                xs_ = torch.clone(xs) \n",
    "                xs= xs.detach().numpy() # (100,2000)\n",
    "                xt_ = torch.clone(xt) \n",
    "                xt= xt.detach().numpy() #(300,2000)\n",
    "                istensor = True\n",
    "                if ws is not None:\n",
    "                    ws = ws.numpy().astype(np.float64)\n",
    "                else :\n",
    "                    ws = np.ones((xs.shape[0],),dtype=np.float64) / xs.shape[0]\n",
    "                \n",
    "                if wt is not None:\n",
    "                    wt = wt.numpy().astype(np.float64)\n",
    "                else :\n",
    "                    wt = np.ones((xt.shape[0],),dtype=np.float64) / xs.shape[0]\n",
    "                \n",
    "            # creating object for interpolation\n",
    "            interp_G = InterpMeas(metric=self.metric,t_val=self.t_val,approx_interp=approx_interp,\n",
    "                                  learn_support=self.learn_support)\n",
    "    \n",
    "            \n",
    "            interp_G.fit(xs,xt,a=wt, b=ws)\n",
    "            G, weight_G, cost_g= interp_G.int_m, interp_G.weights, interp_G.cost\n",
    "            interp_G.int_init = G\n",
    "            self.int_m = interp_G.int_m\n",
    "            self.cost_g = cost_g\n",
    "            # preparing output for differentiable cost\n",
    "            if istensor:\n",
    "                eps = 0\n",
    "    \n",
    "                Ms = euclidean_dist_torch(xs_.double(), xt_.double()).pow(self.p)\n",
    "             \n",
    "                with torch.no_grad():\n",
    "\n",
    "                    Ms_aux =  Ms.detach().data.numpy()\n",
    "                    normMs = np.max(Ms_aux) if np.max(Ms_aux)>1 else 1\n",
    "    \n",
    "                    #print(np.sum(a),np.sum(b),np.sum(c),)\n",
    "                    gamma_s = ot.emd(ws, wt, Ms_aux/normMs)\n",
    "                    planS = torch.from_numpy(gamma_s)\n",
    "    \n",
    "                cost = (torch.sum(Ms*planS)+ eps)**(1/self.p)\n",
    "            \n",
    "                return cost"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:54:07.263669Z",
     "start_time": "2024-03-24T10:54:07.204666Z"
    }
   },
   "id": "88020489c4d8ee18"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(1000, 3073)\n",
      "(1000, 3073)\n"
     ]
    }
   ],
   "source": [
    "def dataloader_to_df(dataloader):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "\n",
    "    for inputs, target in dataloader:\n",
    " \n",
    "        inputs = inputs.numpy().reshape(-1, 32 * 32 * 3)\n",
    "        target = target.numpy()\n",
    "    \n",
    "        # 添加数据到列表\n",
    "        data.append(inputs)\n",
    "        labels.append(target)\n",
    "    \n",
    " \n",
    "    data = np.vstack(data)\n",
    "    labels = np.concatenate(labels)\n",
    "    df = pd.DataFrame(data)\n",
    "    df['label'] = labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# 加载训练集\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# 加载测试集\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# 定义类别标签\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "df_train = dataloader_to_df(trainloader)[:1000]\n",
    "df_test = dataloader_to_df(testloader)[:1000]\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:54:18.200385Z",
     "start_time": "2024-03-24T10:54:07.769941Z"
    }
   },
   "id": "5f071d5784b121ef"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_data(x,y):\n",
    "    # index_list = torch.argmax(torch.Tensor(y), dim=1)\n",
    "    \n",
    "    numpy_data = np.array(x)\n",
    "    numpy_labels = np.array(y)\n",
    "\n",
    "    dim = x.shape[1]\n",
    "    assert len(numpy_data) == len(numpy_labels)\n",
    "\n",
    "    data_with_labels = [(data, label) for data, label in zip(numpy_data, numpy_labels)]\n",
    "\n",
    "    batch_size = 32  \n",
    "    dataset = DatasetSplit(data_with_labels,numpy_data,numpy_labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    targets1 = dataset.targets\n",
    "    vals1, cts1 = torch.unique(targets1, return_counts=True)\n",
    "    min_labelcount = 2\n",
    "    V1 = torch.sort(vals1[cts1 >= min_labelcount])[0]\n",
    "    idxs1 = np.array([i for i in range(len(targets1))])\n",
    "    classes1 = vals1\n",
    "    \n",
    "    M1, C1 = compute_label_stats(data_loader, targets1, idxs1, classes1, diagonal_cov=True)\n",
    "    # print(M1.shape)\n",
    "    # print(C1.shape)\n",
    "    DA = (dataset.dataset.view(-1,dim).to(device), dataset.targets.to(device))\n",
    "    XA = augmented_dataset(DA, means=M1, covs=C1, maxn=10000)\n",
    "    \n",
    "    return XA\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "\n",
    "    def __init__(self, data_with_labels,numpy_data,label):\n",
    "        self.data_with_labels = data_with_labels\n",
    "        self.dataset = torch.Tensor(numpy_data)\n",
    "        self.targets = torch.LongTensor(label)\n",
    "        self.idxs = np.array([i for i in range(len(label))])\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data_with_labels[index]\n",
    "        return image, label\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:54:18.248246Z",
     "start_time": "2024-03-24T10:54:18.204696Z"
    }
   },
   "id": "59661c81e9c4694c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_gaussian_noise_cifar10(data, num_samples, noise_mean=0, noise_std=25):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to CIFAR-10 data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array of shape (N, 3072) where N is the number of samples\n",
    "    - num_samples: number of samples to add noise to\n",
    "    - noise_mean: mean of the Gaussian noise (default: 0)\n",
    "    - noise_std: standard deviation of the Gaussian noise (default: 25)\n",
    "\n",
    "    Returns:\n",
    "    - numpy array of shape (N, 3072) with added Gaussian noise\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples = min(num_samples, len(data)) \n",
    "    sample_indices = random.sample(range(len(data)), num_samples)\n",
    "\n",
    "\n",
    "    noisy_data = data.copy()\n",
    "    for idx in sample_indices:\n",
    "        noise = np.random.normal(noise_mean, noise_std, noisy_data[idx].shape)\n",
    "        noisy_sample = noisy_data[idx] + noise\n",
    "        \n",
    "        # 确保像素值在 0 到 255 之间\n",
    "        noisy_sample = np.clip(noisy_sample, 0, 255)\n",
    "\n",
    "        noisy_data[idx] = noisy_sample.astype(np.uint8) \n",
    "\n",
    "    return noisy_data,sample_indices\n",
    "\n",
    "\n",
    "def cal_distance(xs_t,xt_t,support=200,n_epoch=50,metric='euclidean',t_val=0.5):\n",
    "\n",
    "    \n",
    "    xs = xs_t.numpy()\n",
    "    xt = xt_t.numpy()\n",
    "\n",
    "\n",
    "    k,dim = xs.shape[0],xs.shape[1]\n",
    "    print('sample size {}'.format(k))\n",
    "    print('xs.shape',xs.shape)\n",
    "    print('xt.shape',xt.shape)\n",
    "    print('Support size',support)\n",
    "    \n",
    "    fedot_st = FedOT(n_supp=support,n_epoch=n_epoch,metric=metric,t_val=t_val)\n",
    "    \n",
    "    fedot_start = time.time()\n",
    "    fedot_st.fit(xs_t,xt_t)\n",
    "    st_distance = fedot_st.cost\n",
    "    fedot_end = time.time()\n",
    "    print('time',fedot_end - fedot_start)\n",
    "\n",
    "    return st_distance,fedot_st.int_meas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:54:21.996842Z",
     "start_time": "2024-03-24T10:54:21.930466Z"
    }
   },
   "id": "9ef7168800aebea6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in MOMENTS:  False\n",
      "in MOMENTS:  False\n",
      "in MOMENTS:  False\n",
      "in MOMENTS:  False\n",
      "torch.Size([100, 9216])\n",
      "torch.Size([100, 9216])\n",
      "torch.Size([100, 9216])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "sample_num = 100\n",
    "train_x, train_y = df_train.iloc[:sample_num,:-1],  df_train.iloc[:sample_num,-1]\n",
    "test_x, test_y = df_test.iloc[:sample_num,:-1],  df_test.iloc[:sample_num,-1]\n",
    "noisy_train_x,noisy_index = add_gaussian_noise_cifar10(train_x, num_samples=20, noise_mean=2, noise_std=25)\n",
    "\n",
    "noisy2_train_x,noisy2_index = add_gaussian_noise_cifar10(train_x, num_samples=50, noise_mean=2, noise_std=25)\n",
    "\n",
    "XA = process_data(train_x,train_y)\n",
    "XT = process_data(test_x,test_y)\n",
    "XD = XA \n",
    "X_noise = process_data(noisy_train_x,train_y)\n",
    "X_noise2 = process_data(noisy2_train_x,train_y)\n",
    "\n",
    "print(XA.shape)\n",
    "print(XT.shape)\n",
    "print(X_noise.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:54:23.236124Z",
     "start_time": "2024-03-24T10:54:22.993384Z"
    }
   },
   "id": "d465ebefbcc8a4eb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- evaluate XA --------------- \n",
      "sample size 100\n",
      "xs.shape (100, 9216)\n",
      "xt.shape (100, 9216)\n",
      "Support size 100\n",
      "time 5.950355052947998\n",
      "--------------- evaluate X_noise --------------- \n",
      "sample size 100\n",
      "xs.shape (100, 9216)\n",
      "xt.shape (100, 9216)\n",
      "Support size 100\n",
      "time 2.649045944213867\n",
      "--------------- evaluate X_noise2 --------------- \n",
      "sample size 100\n",
      "xs.shape (100, 9216)\n",
      "xt.shape (100, 9216)\n",
      "Support size 100\n",
      "time 2.4271059036254883\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t_val = 0.5\n",
    "candidate_ls = [XA,X_noise,X_noise2]\n",
    "candidate_item = ['XA','X_noise','X_noise2']\n",
    "results = {}\n",
    "\n",
    "for i in range(len(candidate_ls)):\n",
    "    test_data_name = str(candidate_item[i])\n",
    "    print('--------------- evaluate {} --------------- '.format(test_data_name))\n",
    "    XS = candidate_ls[i]\n",
    "    distance, int_meas= cal_distance(XS,XT,support=XS.shape[0],n_epoch=50,metric='euclidean',t_val=0.5)\n",
    "    results[test_data_name] = [distance, int_meas]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:54:35.595946Z",
     "start_time": "2024-03-24T10:54:24.524181Z"
    }
   },
   "id": "176668cf646233a3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:57:13.488329Z",
     "start_time": "2024-03-24T10:57:13.376563Z"
    }
   },
   "id": "b520aa11a3556b"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance of XA is 33.459589179559\n",
      "Distance of X_noise is 124.44349025740004\n",
      "Distance of X_noise2 is 184.37669944485592\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(candidate_item)):\n",
    "    print('Distance of {} is {}'.format(candidate_item[i],results[candidate_item[i]][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:57:13.744116Z",
     "start_time": "2024-03-24T10:57:13.667928Z"
    }
   },
   "id": "60218d65d5a94bb9"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lava accuracy 0.58 with count 29\n",
      "FedBary accuracy 0.58 with count 29\n"
     ]
    }
   ],
   "source": [
    "from geomloss.samples_loss import *\n",
    "loss = SamplesLoss(\n",
    "        loss='sinkhorn', p=2,\n",
    "        # cost=cost_geomloss,\n",
    "        debias=True,\n",
    "        blur=0.1 ** (1 / 2),\n",
    "        backend='tensorized',\n",
    "        potentials=True\n",
    "    )\n",
    "\n",
    "def values(dual_sol, training_size):\n",
    "    dualsol = dual_sol\n",
    "\n",
    "    f1k = np.array(dualsol.squeeze())\n",
    "\n",
    "    trainGradient = (1 + 1 / (training_size - 1)) * f1k - sum(f1k) / (training_size - 1)\n",
    "    return list(trainGradient)\n",
    "\n",
    "\n",
    "def point_detect(xs_init,xt_init,noisy_index):\n",
    "    \n",
    "    dual = np.array(loss(xs_init.double(), xt_init.double())[0])\n",
    "    \n",
    "    value = values( torch.from_numpy(dual), len(XD))\n",
    "    counts = np.where( np.array(value)>0)[0]\n",
    "    count_k=0\n",
    "    for item in counts:\n",
    "        if item in noisy_index:\n",
    "            count_k +=1\n",
    "    accuracy = count_k/len(noisy_index)\n",
    "    count = count_k\n",
    "    \n",
    "    return accuracy, count\n",
    "\n",
    "xs_init = X_noise2\n",
    "xt_init=  XT\n",
    "accuracy, count  = point_detect(X_noise2,xt_init,noisy2_index)\n",
    "\n",
    "print('Lava accuracy {} with count {}'.format(accuracy,count))\n",
    "\n",
    "xs_init = X_noise2\n",
    "xt_init=  torch.from_numpy( results['X_noise2'][1])\n",
    "accuracy, count  = point_detect(X_noise2,xt_init,noisy2_index)\n",
    "\n",
    "print('FedBary accuracy {} with count {}'.format(accuracy,count))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T10:57:15.595865Z",
     "start_time": "2024-03-24T10:57:15.428937Z"
    }
   },
   "id": "c0e6e8bd07a8d843"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "37dbd3256ae94970"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "76a0a95040b481d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
